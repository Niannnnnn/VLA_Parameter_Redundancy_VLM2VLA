import torch
import os
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import re
import pandas as pd
from prismatic import load
import csv

def extract_model_info(model, model_name):
    """æå–æ¨¡å‹çš„ç»“æ„å’Œå‚æ•°ä¿¡æ¯"""
    try:
        # è·å–æ¨¡å‹ç»“æ„
        structure = {
            'name': model_name,
            'num_parameters': sum(p.numel() for p in model.parameters()),  # æ€»å‚æ•°æ•°é‡
            'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad),  # å¯è®­ç»ƒå‚æ•°æ•°é‡
        }
        return structure
    except Exception as e:
        print(f"æå–æ¨¡å‹ä¿¡æ¯æ—¶å‡ºé”™ï¼š{e}")
        return None

def show_model_params(model, save_path):
    # print(f"æ¨¡å‹å‚æ•°åç§°ä¸å°ºå¯¸å¦‚ä¸‹ï¼š")
    state_dict = model.state_dict()
    # for name, param in state_dict.items():
    #     print(f"{name:60s} {tuple(param.shape)}")

    with open(save_path, "w", encoding="utf-8") as f:
        f.write(str(model))

    
    print(f"âœ… å‚æ•°ä¿¡æ¯å·²ä¿å­˜åˆ° {save_path}")

def weight_mapping(model_orig, model_finetuned, save_path):
    plm_state = model_orig.state_dict()
    olm_state = model_finetuned.state_dict()

    # å®šä¹‰åŒ¹é…å‰ç¼€æ˜ å°„å…³ç³»
    prefix_map = [
        ("llm_backbone.llm.model.layers.", "module.language_model.model.layers."),
    ]

    # å®šä¹‰æˆ‘ä»¬å…³å¿ƒçš„æƒé‡å…³é”®å­—
    target_suffixes = [
        "self_attn.q_proj.weight",
        "self_attn.k_proj.weight",
        "self_attn.v_proj.weight",
        "self_attn.o_proj.weight",
        "mlp.up_proj.weight",
        "mlp.gate_proj.weight",
        "mlp.down_proj.weight",
        # å¦‚æœè¿˜æƒ³ä¿ç•™ layernormï¼Œå¯åŠ ä¸Šï¼š
        # "input_layernorm.weight",
        # "post_attention_layernorm.weight",
    ]

    mapping_list = []

    for plm_name in plm_state.keys():
        # åªè€ƒè™‘ weight
        if not plm_name.endswith("weight"):
            continue

        # åªåŒ¹é…æˆ‘ä»¬å…³å¿ƒçš„å±‚ï¼ˆq,k,v,o,up,gate,downï¼‰
        if not any(suffix in plm_name for suffix in target_suffixes):
            continue

        for plm_prefix, olm_prefix in prefix_map:
            if plm_name.startswith(plm_prefix):
                olm_name = plm_name.replace(plm_prefix, olm_prefix)
                if olm_name in olm_state:
                    mapping_list.append((plm_name, olm_name))
                else:
                    mapping_list.append((plm_name, olm_name + "   # NOT FOUND"))
                break

    # ä¿å­˜ç»“æœ
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    with open(save_path, "w", encoding="utf-8") as f:
        for plm_name, olm_name in mapping_list:
            f.write(f"{plm_name}   ->   {olm_name}\n")

    print(f"âœ… ç²¾ç¡®æƒé‡æ˜ å°„å®Œæˆï¼Œå…± {len(mapping_list)} ä¸ªåŒ¹é…é¡¹ã€‚å·²ä¿å­˜è‡³: {save_path}")
    return mapping_list

def match_parameters(model_orig, model_finetuned, output_dir):
    print("æ­£åœ¨åŒ¹é…å‚æ•°...")
    # è·å–ä¸¤ä¸ªæ¨¡å‹çš„çŠ¶æ€å­—å…¸
    orig_state_dict = model_orig.state_dict()
    # å¤„ç† DataParallel æ¨¡å‹çš„çŠ¶æ€å­—å…¸
    if hasattr(model_finetuned, 'module'):
        finetuned_state_dict = model_finetuned.module.state_dict()
    else:
        finetuned_state_dict = model_finetuned.state_dict()

    print(f"åŸå§‹æ¨¡å‹å‚æ•°æ•°é‡: {len(orig_state_dict)}, å¾®è°ƒæ¨¡å‹å‚æ•°æ•°é‡: {len(finetuned_state_dict)}")
    show_model_params(model_orig, os.path.join(output_dir, "model_orig.txt"))
    show_model_params(model_finetuned, os.path.join(output_dir, "model_finetuned.txt"))

    mapping_list = weight_mapping(model_orig, model_finetuned, os.path.join(output_dir, "prismatic_openvla_mapping.txt"))

    return mapping_list

def compute_vlm_ffn_magnitude(mapping_list, model_orig, output_dir, compute_device="cuda:0"):
    print("ğŸ” æ­£åœ¨è®¡ç®— FFN æƒé‡å¼ºåº¦ (W_VLM)...")
    device = torch.device(compute_device)
    plm_state = model_orig.state_dict()
    ffn_magnitudes = {}

    for (plm_name, olm_name) in mapping_list:
        if "# NOT FOUND" in plm_name: continue
        if not any(x in plm_name for x in ["up_proj", "gate_proj", "down_proj"]): continue

        W_orig = plm_state[plm_name].to(device).float()
        if any(x in plm_name for x in ["up_proj", "gate_proj"]):
            mag = torch.norm(W_orig, p=2, dim=1)
        else:
            mag = torch.norm(W_orig, p=2, dim=0)
        ffn_magnitudes[plm_name] = mag.detach().cpu()

    os.makedirs(output_dir, exist_ok=True)
    # å®Œæ•´ç‰ˆ
    save_path = os.path.join(output_dir, "ffn_vlm_magnitude_l2.txt")
    with open(save_path, "w", encoding="utf-8") as f:
        f.write("Layer_Name | Min_L2 | Max_L2 | Mean_L2 | Channel_Values\n")
        f.write("-" * 80 + "\n")
        for layer_name, mag_tensor in ffn_magnitudes.items():
            m_min, m_max, m_mean = mag_tensor.min(), mag_tensor.max(), mag_tensor.mean()
            val_str = ",".join([f"{x:.6f}" for x in mag_tensor.tolist()])
            f.write(f"{layer_name} | {m_min:.6f} | {m_max:.6f} | {m_mean:.6f} | {val_str}\n")
    # ç®€ç•¥ç‰ˆ
    save_path_short = os.path.join(output_dir, "ffn_vlm_magnitude_l2_short.txt")
    with open(save_path_short, "w", encoding="utf-8") as f:
        f.write("Layer_Name | Min_L2 | Max_L2 | Mean_L2\n")
        f.write("-" * 50 + "\n")
        for layer_name, mag_tensor in ffn_magnitudes.items():
            f.write(f"{layer_name} | {mag_tensor.min():.6f} | {mag_tensor.max():.6f} | {mag_tensor.mean():.6f}\n")
    return ffn_magnitudes

def compute_vla_ffn_magnitude(mapping_list, model_finetuned, output_dir, compute_device="cuda:0"):
    print("ğŸ” æ­£åœ¨è®¡ç®— FFN æƒé‡å¼ºåº¦ (W_VLA)...")
    device = torch.device(compute_device)
    olm_state = model_finetuned.state_dict()
    ffn_magnitudes = {}

    for (plm_name, olm_name) in mapping_list:
        if "# NOT FOUND" in olm_name: continue
        if not any(x in olm_name for x in ["up_proj", "gate_proj", "down_proj"]): continue

        W_fine = olm_state[olm_name].to(device).float()
        if any(x in olm_name for x in ["up_proj", "gate_proj"]):
            mag = torch.norm(W_fine, p=2, dim=1)
        else:
            mag = torch.norm(W_fine, p=2, dim=0)
        ffn_magnitudes[olm_name] = mag.detach().cpu()

    os.makedirs(output_dir, exist_ok=True)
    save_path = os.path.join(output_dir, "ffn_vla_magnitude_l2.txt")
    with open(save_path, "w", encoding="utf-8") as f:
        f.write("Layer_Name | Min_L2 | Max_L2 | Mean_L2 | Channel_Values\n")
        f.write("-" * 80 + "\n")
        for layer_name, mag_tensor in ffn_magnitudes.items():
            m_min, m_max, m_mean = mag_tensor.min(), mag_tensor.max(), mag_tensor.mean()
            val_str = ",".join([f"{x:.6f}" for x in mag_tensor.tolist()])
            f.write(f"{layer_name} | {m_min:.6f} | {m_max:.6f} | {m_mean:.6f} | {val_str}\n")
    save_path_short = os.path.join(output_dir, "ffn_vla_magnitude_l2_short.txt")
    with open(save_path_short, "w", encoding="utf-8") as f:
        f.write("Layer_Name | Min_L2 | Max_L2 | Mean_L2\n")
        f.write("-" * 50 + "\n")
        for layer_name, mag_tensor in ffn_magnitudes.items():
            f.write(f"{layer_name} | {mag_tensor.min():.6f} | {mag_tensor.max():.6f} | {mag_tensor.mean():.6f}\n")
    return ffn_magnitudes

def compute_ffn_delta_magnitude(mapping_list, model_orig, model_finetuned, output_dir, compute_device="cuda:0"):
    print("ğŸ” æ­£åœ¨è®¡ç®— FFN æƒé‡å˜åŒ–å‰§çƒˆç¨‹åº¦ (L2 of Delta W)...")
    device = torch.device(compute_device)
    plm_state = model_orig.state_dict()
    olm_state = model_finetuned.state_dict()
    ffn_deltas = {}

    for (plm_name, olm_name) in mapping_list:
        if "# NOT FOUND" in olm_name: continue
        if not any(x in olm_name for x in ["up_proj", "gate_proj", "down_proj"]): continue

        W_orig = plm_state[plm_name].to(device).float()
        W_fine = olm_state[olm_name].to(device).float()
        delta_W = W_fine - W_orig
        if any(x in olm_name for x in ["up_proj", "gate_proj"]):
            delta_mag = torch.norm(delta_W, p=2, dim=1)
        else:
            delta_mag = torch.norm(delta_W, p=2, dim=0)
        ffn_deltas[olm_name] = delta_mag.detach().cpu()

    os.makedirs(output_dir, exist_ok=True)
    save_path = os.path.join(output_dir, "ffn_delta_magnitude_l2.txt")
    with open(save_path, "w", encoding="utf-8") as f:
        f.write("Layer_Name | Min_Delta_L2 | Max_Delta_L2 | Mean_Delta_L2 | Delta_Values\n")
        f.write("-" * 80 + "\n")
        for layer_name, d_tensor in ffn_deltas.items():
            val_str = ",".join([f"{x:.6f}" for x in d_tensor.tolist()])
            f.write(f"{layer_name} | {d_tensor.min():.6f} | {d_tensor.max():.6f} | {d_tensor.mean():.6f} | {val_str}\n")
    save_path_short = os.path.join(output_dir, "ffn_delta_magnitude_l2_short.txt")
    with open(save_path_short, "w", encoding="utf-8") as f:
        f.write("Layer_Name | Min_Delta_L2 | Max_Delta_L2 | Mean_Delta_L2\n")
        f.write("-" * 50 + "\n")
        for layer_name, d_tensor in ffn_deltas.items():
            f.write(f"{layer_name} | {d_tensor.min():.6f} | {d_tensor.max():.6f} | {d_tensor.mean():.6f}\n")
    return ffn_deltas

def compute_ffn_cosine_similarity(mapping_list, model_orig, model_finetuned, output_dir, compute_device="cuda:0"):
    print("ğŸ” æ­£åœ¨è®¡ç®— FFN æƒé‡æ–¹å‘å˜åŒ– (Cosine Similarity)...")
    device = torch.device(compute_device)
    plm_state = model_orig.state_dict()
    olm_state = model_finetuned.state_dict()
    ffn_cosines = {}

    for (plm_name, olm_name) in mapping_list:
        if "# NOT FOUND" in olm_name: continue
        if not any(x in olm_name for x in ["up_proj", "gate_proj", "down_proj"]): continue

        W_orig = plm_state[plm_name].to(device).float()
        W_fine = olm_state[olm_name].to(device).float()

        # æŒ‰è¡Œæˆ–åˆ—è®¡ç®— Cosine Similarity
        if any(x in olm_name for x in ["up_proj", "gate_proj"]):
            # è¡Œå‘é‡
            W_orig_flat = W_orig / (W_orig.norm(p=2, dim=1, keepdim=True) + 1e-8)
            W_fine_flat = W_fine / (W_fine.norm(p=2, dim=1, keepdim=True) + 1e-8)
            cos_sim = (W_orig_flat * W_fine_flat).sum(dim=1)
        else:
            # åˆ—å‘é‡
            W_orig_flat = W_orig / (W_orig.norm(p=2, dim=0, keepdim=True) + 1e-8)
            W_fine_flat = W_fine / (W_fine.norm(p=2, dim=0, keepdim=True) + 1e-8)
            cos_sim = (W_orig_flat * W_fine_flat).sum(dim=0)

        ffn_cosines[olm_name] = cos_sim.detach().cpu()

    os.makedirs(output_dir, exist_ok=True)
    save_path = os.path.join(output_dir, "ffn_cosine_similarity.txt")
    with open(save_path, "w", encoding="utf-8") as f:
        f.write("Layer_Name | Min_Cos | Max_Cos | Mean_Cos | Cos_Values\n")
        f.write("-" * 80 + "\n")
        for layer_name, c_tensor in ffn_cosines.items():
            val_str = ",".join([f"{x:.6f}" for x in c_tensor.tolist()])
            f.write(f"{layer_name} | {c_tensor.min():.6f} | {c_tensor.max():.6f} | {c_tensor.mean():.6f} | {val_str}\n")
    save_path_short = os.path.join(output_dir, "ffn_cosine_similarity_short.txt")
    with open(save_path_short, "w", encoding="utf-8") as f:
        f.write("Layer_Name | Min_Cos | Max_Cos | Mean_Cos\n")
        f.write("-" * 50 + "\n")
        for layer_name, c_tensor in ffn_cosines.items():
            f.write(f"{layer_name} | {c_tensor.min():.6f} | {c_tensor.max():.6f} | {c_tensor.mean():.6f}\n")
    return ffn_cosines

def analyze_channel_diff(vlm_data, vla_data, delta_data, cos_data, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    proj_types = ["gate_proj", "up_proj", "down_proj"]

    # è¾…åŠ©å‡½æ•°ï¼šæ ¹æ®å±‚å·å’Œç±»å‹æ‰¾åˆ°æ­£ç¡®çš„ Key
    def get_key_by_idx(data_dict, layer_idx, proj_type):
        for k in data_dict.keys():
            # ä½¿ç”¨æ­£åˆ™æˆ–æ›´ä¸¥è°¨çš„åˆ¤æ–­ï¼Œç¡®ä¿åŒ¹é…åˆ°å¯¹åº”çš„å±‚å’ŒæŠ•å½±ç±»å‹
            # åŒ¹é…åŒ…å« .layers.{idx}. ä¸”åŒ…å« proj_type çš„ key
            if f".layers.{layer_idx}." in k and proj_type in k:
                return k
        return None

    # è‡ªåŠ¨æå– vlm_data ä¸­æ‰€æœ‰çš„å±‚å·
    all_layers = []
    for k in vlm_data.keys():
        match = re.search(r'layers\.(\d+)\.', k)
        if match:
            all_layers.append(int(match.group(1)))
    
    unique_layers = sorted(list(set(all_layers)))

    for proj in proj_types:
        layers_axis = []
        vlm_means, vla_means, delta_means, cos_means = [], [], [], []

        for idx in unique_layers:
            # å°è¯•åœ¨å››ä¸ªæ•°æ®æºä¸­å®šä½å¯¹åº”çš„ Key
            k_vlm = get_key_by_idx(vlm_data, idx, proj)
            k_vla = get_key_by_idx(vla_data, idx, proj)
            k_delta = get_key_by_idx(delta_data, idx, proj)
            k_cos = get_key_by_idx(cos_data, idx, proj)

            # åªæœ‰å½“å››ä¸ªæ–‡ä»¶éƒ½å­˜åœ¨è¯¥å±‚æ•°æ®æ—¶æ‰è¿›è¡Œæ”¶é›†
            if all([k_vlm, k_vla, k_delta, k_cos]):
                layers_axis.append(idx)
                vlm_means.append(vlm_data[k_vlm].mean().item())
                vla_means.append(vla_data[k_vla].mean().item()) # å·²ä¿®æ­£ï¼šåªæ·»åŠ ä¸€æ¬¡
                delta_means.append(delta_data[k_delta].mean().item())
                cos_means.append(cos_data[k_cos].mean().item())

        if not layers_axis:
            print(f"âš ï¸ è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°å±äº {proj} çš„åŒ¹é…æ•°æ®ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
            continue

        # --- ç»˜å›¾é€»è¾‘ ---
        plt.figure(figsize=(12, 6))
        
        # ç»˜åˆ¶å››æ¡å¯¹æ¯”æ›²çº¿
        plt.plot(layers_axis, vlm_means, label='VLM (Original) Mean L2', marker='o', alpha=0.8, color='#1f77b4')
        plt.plot(layers_axis, vla_means, label='VLA (Finetuned) Mean L2', marker='s', alpha=0.8, color='#ff7f0e')
        plt.plot(layers_axis, delta_means, label='Delta (Weight Change) L2', marker='^', alpha=0.8, color='#2ca02c')
        plt.plot(layers_axis, cos_means, label='Cosine Similarity', marker='x', color='#d62728', linestyle='--')

        plt.title(f"FFN Channel Importance Analysis: {proj.upper()}", fontsize=14)
        plt.xlabel("Layer Index", fontsize=12)
        plt.ylabel("Score / Magnitude", fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.legend(loc='best')
        
        # ä¼˜åŒ– X è½´åˆ»åº¦æ˜¾ç¤º
        if len(layers_axis) > 10:
            plt.xticks(layers_axis[::2]) # å¦‚æœå±‚æ•°å¤ªå¤šï¼Œæ¯éš”ä¸€å±‚æ˜¾ç¤ºä¸€ä¸ªæ ‡ç­¾
        else:
            plt.xticks(layers_axis)

        plt.tight_layout()
        save_path = os.path.join(output_dir, f"ffn_analysis_{proj}.png")
        plt.savefig(save_path, dpi=200)
        plt.close()
        print(f"âœ… å·²ç”Ÿæˆåˆ†æå›¾è¡¨: {save_path}")


import matplotlib.pyplot as plt
import seaborn as sns

def plot_channel_score_distribution(flat_scores, output_dir):
    """
    æ”¹è¿›ç‰ˆï¼šèšç„¦ä¸»ä½“åˆ†å¸ƒï¼Œè‡ªåŠ¨å‰”é™¤è¿œç«¯ç¦»ç¾¤å€¼ï¼ˆOutliersï¼‰
    """
    import numpy as np
    scores_np = flat_scores.detach().cpu().numpy()
    
    # --- æ ¸å¿ƒæ”¹è¿›ï¼šè®¡ç®— 99.5% åˆ†ä½æ•°ï¼Œè¿‡æ»¤æ‰æå°‘æ•°æå¤§å€¼ ---
    # è¿™æ ·å¯ä»¥ç¡®ä¿æ¨ªåæ ‡èšç„¦åœ¨ 0 åˆ°ç»å¤§å¤šæ•°æ•°æ®æ‰€åœ¨çš„èŒƒå›´
    upper_limit = np.percentile(scores_np, 99.99) 
    filtered_scores = scores_np[scores_np <= upper_limit]

    plt.figure(figsize=(10, 6))
    
    # ä½¿ç”¨æ›´ç»†è…»çš„ bins=150 è®©å±±å³°æ›´å¹³æ»‘
    sns.histplot(filtered_scores, kde=True, color='royalblue', bins=150, alpha=0.6)
    
    # åŠ¨æ€è®¾ç½® x è½´èŒƒå›´ï¼Œç¨å¾®ç•™ç™½
    plt.xlim(left=0, right=upper_limit * 1.05)
    
    plt.xlabel("Scores of Channels", fontsize=12)
    plt.ylabel("Frequency / Density", fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.5)
    
    # æ·»åŠ ä¸€ä¸ªç®€å•çš„æ–‡æœ¬è¯´æ˜ï¼Œå‘ŠçŸ¥ç¦»ç¾¤å€¼æƒ…å†µ
    # num_outliers = len(scores_np) - len(filtered_scores)
    # plt.text(upper_limit * 0.7, plt.ylim()[1] * 0.8, 
    #          f"Outliers excluded: {num_outliers}\nMax score: {scores_np.max():.2f}", 
    #          bbox=dict(facecolor='white', alpha=0.5))

    plot_path = os.path.join(output_dir, "ffn_score_main_distribution.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"ğŸ“Š ä¸»ä½“åˆ†å¸ƒå›¾å·²ä¿å­˜ï¼ˆå·²è¿‡æ»¤æå¤§å€¼ï¼‰: {plot_path}")

def prune_channel(vlm_data, vla_data, delta_data, cos_data, output_dir, ratio=0.2, by_smallest=True):
    """
    çº¯ Delta å‰ªæé€»è¾‘ï¼š
    åªåˆ©ç”¨ delta_data (å¾®è°ƒæƒé‡å·®å¼‚) ä½œä¸ºè¯„åˆ†æ ‡å‡†ã€‚
    by_smallest=True: å‰ªæ‰å˜åŒ–æœ€å°çš„ (Deltaæœ€å°, ä¿ç•™æ–°çŸ¥è¯†)
    by_smallest=False: å‰ªæ‰å˜åŒ–æœ€å¤§çš„ (Deltaæœ€å¤§, å‰”é™¤å˜åŠ¨æœ€å‰§çƒˆçš„)
    """
    pruning_masks = {}
    all_channel_scores = []
    layer_metadata = {}

    # 1. æå–æ‰€æœ‰å±‚å·
    all_keys = list(delta_data.keys())
    unique_layers = sorted(list(set([
        int(re.search(r'layers\.(\d+)\.', k).group(1)) 
        for k in all_keys if re.search(r'layers\.(\d+)\.', k)
    ])))

    print(f"\n[å‰ªæé…ç½®] ç›®æ ‡æ¯”ä¾‹: {ratio:.2%}")
    print(f"[å‰ªæé…ç½®] è¯„åˆ†é€»è¾‘: ä»…ä½¿ç”¨ Delta_Magnitude (L2å·®å¼‚)")
    print(f"[å‰ªæé…ç½®] å‰ªææ–¹å‘: {'å‰ªæœ€å° (Smallest Delta)' if by_smallest else 'å‰ªæœ€å¤§ (Largest Delta)'}")

    # 2. è®¡ç®—å¾—åˆ† (ä»…ä½¿ç”¨ delta_data)
    for idx in unique_layers:
        k_gate = next((k for k in delta_data.keys() if f".layers.{idx}." in k and "gate_proj" in k), None)
        k_up = next((k for k in delta_data.keys() if f".layers.{idx}." in k and "up_proj" in k), None)
        k_down = next((k for k in delta_data.keys() if f".layers.{idx}." in k and "down_proj" in k), None)
        
        if not all([k_gate, k_up, k_down]): continue
        
        is_protected = (idx <= 4 or idx >= 30)
        
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šåªè®¡ç®— Delta çš„å¼ºåº¦ ---
        # èåˆ gate/up/down ä¸‰å±‚çš„ä¸€è‡´æ€§å·®å¼‚
        delta_intensity = delta_data[k_gate] + delta_data[k_up] + delta_data[k_down]
        
        # è¿™é‡Œçš„å¾—åˆ†å°±æ˜¯ Delta çš„é‡çº§
        combined_scores = delta_intensity

        layer_metadata[idx] = {
            'keys': (k_gate, k_up, k_down),
            'num_channels': len(combined_scores),
            'is_protected': is_protected,
            'scores': combined_scores
        }

        if not is_protected:
            all_channel_scores.append(combined_scores)

    # 3. è®¡ç®—å…¨å±€é˜ˆå€¼
    if not all_channel_scores:
        print("âŒ é”™è¯¯ï¼šæ²¡æœ‰å¯å‰ªæå±‚")
        return {}
        
    flat_scores = torch.cat(all_channel_scores)

    # è°ƒç”¨ç»˜å›¾å‡½æ•°ï¼ˆç°åœ¨ç”»çš„æ˜¯ Delta çš„åˆ†å¸ƒï¼‰
    plot_channel_score_distribution(flat_scores, output_dir)
    
    # ç¡®å®šé˜ˆå€¼
    if by_smallest:
        # å‰ªæ‰å˜åŠ¨æœ€å°çš„ï¼Œä¿ç•™ >= threshold
        num_to_prune_global = int(len(flat_scores) * ratio)
        sorted_scores, _ = torch.sort(flat_scores)
        threshold = sorted_scores[num_to_prune_global].item()
    else:
        # å‰ªæ‰å˜åŠ¨æœ€å¤§çš„ï¼Œä¿ç•™ <= threshold
        num_to_prune_global = int(len(flat_scores) * (1 - ratio))
        sorted_scores, _ = torch.sort(flat_scores)
        threshold = sorted_scores[num_to_prune_global].item()

    print(f"[é…ç½®] Delta å¼ºåº¦é˜ˆå€¼: {threshold:.6e}")
    print(f"{'å±‚å·':<10} | {'çŠ¶æ€':<10} | {'å‰ªé™¤æ•°':<10} | {'å±€éƒ¨æ¯”ä¾‹':<10}")
    print("-" * 55)

    # 4. ç”Ÿæˆæ©ç 
    for idx in unique_layers:
        if idx not in layer_metadata: continue
        meta = layer_metadata[idx]
        
        k_gate = meta['keys'][0].replace("module.", "")
        k_up = meta['keys'][1].replace("module.", "")
        k_down = meta['keys'][2].replace("module.", "")
        total_len = meta['num_channels']
        
        if meta['is_protected']:
            mask = np.ones(total_len, dtype=bool)
            status = "PROTECTED"
        else:
            if by_smallest:
                mask = (meta['scores'] >= threshold).numpy()
            else:
                mask = (meta['scores'] <= threshold).numpy()
            status = "PRUNABLE"

        pruned_count = int(np.sum(~mask))
        print(f"Layer {idx:<5} | {status:<10} | {pruned_count:<10d} | {pruned_count/total_len:>10.2%}")

        entry = {'layer_num': idx, 'pruned_channels': pruned_count, 'total_channels': total_len}
        pruning_masks[k_gate] = {**entry, 'output_mask': mask, 'input_mask': None, 'layer_type': 'gate'}
        pruning_masks[k_up] = {**entry, 'output_mask': mask, 'input_mask': None, 'layer_type': 'first'}
        pruning_masks[k_down] = {**entry, 'output_mask': None, 'input_mask': mask, 'layer_type': 'down'}

    # 5. ä¿å­˜ç»“æœ
    mode_str = "delta_smallest" if by_smallest else "delta_largest"
    os.makedirs(output_dir, exist_ok=True)
    save_path = os.path.join(output_dir, f"ffn_pruning_masks_ratio_{ratio}_{mode_str}.pth")
    torch.save(pruning_masks, save_path)
    
    return pruning_masks

def load_prismatic_vlm(model_gpu_id=1):
    """ä½¿ç”¨prismaticåº“åŠ è½½åº•å±‚VLMï¼Œæ”¯æŒCPUæˆ–GPUï¼Œä¼˜å…ˆæœ¬åœ°"""
    device_str = f"GPU:{model_gpu_id}" if model_gpu_id != "cpu" else "CPU"
    print(f"æ­£åœ¨ä½¿ç”¨prismaticåº“åŠ è½½åº•å±‚VLM: prism-dinosiglip-224px+7b åˆ° {device_str}...")

    model_id = "prism-dinosiglip-224px+7b"
    local_dir = f"path/to/local/{model_id}"  # æ›¿æ¢ä¸ºå®é™…çš„æœ¬åœ°è·¯å¾„

    try:
        # å°è¯•åŠ è½½HFä»¤ç‰Œ(å¦‚æœæœ‰)
        try:
            with open(".hf_token", "r") as f:
                hf_token = f.read().strip()
        except:
            hf_token = None
            print("æœªæ‰¾åˆ°HFä»¤ç‰Œæ–‡ä»¶ï¼Œå°è¯•æ— ä»¤ç‰ŒåŠ è½½")

        vlm = load(model_id, hf_token=hf_token)


        # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        if model_gpu_id == "cpu":
            vlm.to("cpu")
        else:
            vlm.to(f"cuda:{model_gpu_id}", dtype=torch.bfloat16)

        print(f"æˆåŠŸåŠ è½½ {model_id} åˆ° {device_str}!")
        return vlm

    except Exception as e:
        print(f"é€šè¿‡prismaticåº“åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        print("å°è¯•å¤‡é€‰æ–¹æ³•...")
        return load_siglip_fallback(model_gpu_id)

def load_siglip_fallback(model_gpu_id=1):
    """å¤‡é€‰æ–¹æ³•ï¼šä½¿ç”¨subfolderå‚æ•°åŠ è½½å­ç›®å½•ä¸­çš„æ¨¡å‹ï¼Œæ”¯æŒCPUæˆ–GPU"""
    from transformers import AutoModel
    device_str = f"GPU:{model_gpu_id}" if model_gpu_id != "cpu" else "CPU"
    print(f"å°è¯•ä½¿ç”¨å¤‡é€‰æ–¹æ³•åŠ è½½æ¨¡å‹: siglip-224px+7b åˆ° {device_str}...")
    
    try:
        from transformers import AutoConfig
        config = AutoConfig.from_pretrained("google/siglip-base-patch16-224")
        config.model_type = "siglip"
        model = AutoModel.from_pretrained(
            "TRI-ML/prismatic-vlms",
            subfolder="prism-dinosiglip-224px+7b",
            config=config,
            torch_dtype=torch.float16,
            trust_remote_code=True
        )
        
        if model_gpu_id == "cpu":
            model.to("cpu")
        else:
            model.to(f"cuda:{model_gpu_id}")
            
        return model
    except Exception as e:
        print(f"å¤‡é€‰æ–¹æ³•åŠ è½½å¤±è´¥: {e}")
        print("å°è¯•åŠ è½½å®˜æ–¹SigLIPæ¨¡å‹ä½œä¸ºæ›¿ä»£...")
        
        try:
            # ä¸ä½¿ç”¨device_map='auto'å‚æ•°
            model = AutoModel.from_pretrained(
                "google/siglip-base-patch16-224",
                torch_dtype=torch.float16
            )
            
            if model_gpu_id == "cpu":
                model.to("cpu")
            else:
                model.to(f"cuda:{model_gpu_id}")
                
            return model
        except Exception as e2:
            print(f"åŠ è½½å®˜æ–¹SigLIPæ¨¡å‹ä¹Ÿå¤±è´¥: {e2}")
            
            # æœ€åå°è¯•
            try:
                print("å°è¯•åŠ è½½å…¶ä»–å®˜æ–¹è§†è§‰æ¨¡å‹ä½œä¸ºæ›¿ä»£...")
                model = AutoModel.from_pretrained(
                    "openai/clip-vit-base-patch16",
                    torch_dtype=torch.float16
                )
                
                if model_gpu_id == "cpu":
                    model.to("cpu")
                else:
                    model.to(f"cuda:{model_gpu_id}")
                    
                return model
            except Exception as e3:
                print(f"æ‰€æœ‰è§†è§‰æ¨¡å‹åŠ è½½å°è¯•éƒ½å¤±è´¥: {e3}")
                return None

def load_openvla(model_gpu_id=1):
    """åŠ è½½openvlaï¼Œæ”¯æŒCPUæˆ–GPU"""
    from transformers import AutoModelForVision2Seq

    
    device_str = f"GPU:{model_gpu_id}" if model_gpu_id != "cpu" else "CPU"
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ï¼šopenvla åˆ° {device_str}")
    
    # é¦–å…ˆé‡Šæ”¾æ˜¾å­˜ç¼“å­˜
    torch.cuda.empty_cache()

    # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å¸®åŠ©å†…å­˜ç®¡ç†
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

    try:
        # æ ¹æ®ä¸åŒè®¾å¤‡é€‰æ‹©åŠ è½½ç­–ç•¥
        if model_gpu_id == "cpu":
            print("è­¦å‘Š: åœ¨CPUä¸ŠåŠ è½½å¤§å‹æ¨¡å‹å¯èƒ½ä¼šå¾ˆæ…¢ä¸”å†…å­˜å ç”¨è¾ƒå¤§")
            model = AutoModelForVision2Seq.from_pretrained(
                "openvla/openvla-7b-finetuned-libero-spatial",
                torch_dtype=torch.float32,  # CPUä¸Šä½¿ç”¨float32
                low_cpu_mem_usage=True,
                trust_remote_code=True
            ).to("cpu")
        else:
            # å°è¯•ç›´æ¥åŠ è½½åˆ°æŒ‡å®šGPUï¼Œä¸ä½¿ç”¨flash_attention_2
            model = AutoModelForVision2Seq.from_pretrained(
                "openvla/openvla-7b-finetuned-libero-spatial",
                torch_dtype=torch.bfloat16, 
                low_cpu_mem_usage=True, 
                trust_remote_code=True
            ).to(f"cuda:{model_gpu_id}")
            
            # ä½¿ç”¨ DataParallel å°†æ¨¡å‹åˆ†å¸ƒåˆ°æŒ‡å®šGPU
            model = torch.nn.DataParallel(model, device_ids=[model_gpu_id])
            
        return model
    except RuntimeError as e:
        if "CUDA out of memory" in str(e):
            print(f"GPU:{model_gpu_id} å†…å­˜ä¸è¶³ï¼Œå°è¯•åœ¨CPUä¸ŠåŠ è½½")
            # åœ¨CPUä¸ŠåŠ è½½
            try:
                model = AutoModelForVision2Seq.from_pretrained(
                    "openvla/openvla-7b-finetuned-libero-spatial",
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                ).to("cpu")
                return model
            except Exception as e2:
                print(f"åœ¨CPUä¸ŠåŠ è½½OpenVLAæ¨¡å‹ä¹Ÿå¤±è´¥: {e2}")
                return None
        else:
            print(f"åŠ è½½OpenVLAæ¨¡å‹æ—¶å‡ºé”™: {e}")
            return None


def analyze_and_generate_ffn_masks(model_orig, model_finetuned, compute_device, output_dir, ratio=0.2, by_smallest=True, prune_target="all"):
    """åˆ†ææ¨¡å‹å¹¶ç”Ÿæˆå‰ªè£æ©ç ï¼Œæ”¯æŒé€‰æ‹©å‰ªæç›®æ ‡ (attention/ffn/all)"""
    print("ç¬¬1æ­¥: åŒ¹é…æ¨¡å‹å‚æ•°...")
    # å‡è®¾ match_parameters å’Œ compute_channel_diff å·²ç»å®šä¹‰
    mapping_list = match_parameters(model_orig, model_finetuned, output_dir)
    
    print("\nç¬¬2æ­¥: è®¡ç®—channelç»´åº¦çš„æƒé‡å·®å¼‚...")
    # channel_diffs = compute_channel_diff(mapping_list, model_orig, model_finetuned, output_dir, compute_device)
    vlm_channel = compute_vlm_ffn_magnitude(mapping_list, model_orig, output_dir, compute_device)
    vla_channel = compute_vla_ffn_magnitude(mapping_list, model_finetuned, output_dir, compute_device)
    diff_l2_channel = compute_ffn_delta_magnitude(mapping_list, model_orig, model_finetuned, output_dir, compute_device)
    diff_cos_channel = compute_ffn_cosine_similarity(mapping_list, model_orig, model_finetuned, output_dir, compute_device)


    print("\nç¬¬3æ­¥: åˆ†æé€šé“å·®å¼‚åˆ†å¸ƒ...")
    # analyze_channel_diff(channel_diffs, output_dir)
    analyze_channel_diff(vlm_channel, vla_channel, diff_l2_channel, diff_cos_channel, "plots")

    print("\nç¬¬4æ­¥: ç”Ÿæˆå‰ªè£æ©ç ...")
    pruning_results = prune_channel(vlm_channel, vla_channel, diff_l2_channel, diff_cos_channel, output_dir, ratio=ratio, by_smallest=by_smallest)
    print("ç”Ÿæˆçš„å‰ªææ©ç ä¿¡æ¯:", pruning_results)
    return pruning_results 
    
    

def save_model_weights_info(model, filename):
    """
    å°†æ¨¡å‹æ‰€æœ‰æƒé‡å‚æ•°çš„åç§°ä¸å°ºå¯¸ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶
    :param model: torch.nn.Module
    :param filename: è¾“å‡ºæ–‡ä»¶å
    """
    if model is None:
        print(f"æ¨¡å‹ä¸º Noneï¼Œè·³è¿‡ä¿å­˜ {filename}")
        return

    lines = []
    for name, param in model.named_parameters():
        lines.append(f"{name}  {tuple(param.shape)}")

    with open(filename, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"å·²ä¿å­˜æ¨¡å‹æƒé‡ä¿¡æ¯åˆ° {filename}ï¼Œå…± {len(lines)} æ¡å‚æ•°ã€‚")








def main():
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
    
    # ä¼˜å…ˆé‡Šæ”¾ GPU å†…å­˜
    torch.cuda.empty_cache()
    
    print(f"CUDAæ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}")
    gpu_count = torch.cuda.device_count()
    print(f"å¯ç”¨GPUæ•°é‡: {gpu_count}")
    
    # ä»å‘½ä»¤è¡Œå‚æ•°ä¸­è·å–å‰ªè£æ¯”ä¾‹å’Œå‰ªè£æ¨¡å¼
    import argparse
    parser = argparse.ArgumentParser(description='FFNä¸­é—´ç»´åº¦å‰ªè£å·¥å…·')
    parser.add_argument('--ratio', type=float, default=0.2, help='å‰ªè£æ¯”ä¾‹ï¼ˆ0.0-1.0ï¼‰')
    parser.add_argument('--prune-target', type=str, default='ffn', choices=['attention', 'ffn', 'all'], 
                        help='é€‰æ‹©å‰ªæç›®æ ‡: attention (ä»…å‰ªæattention), ffn (ä»…å‰ªæMLP), all (å…¨éƒ¨å‰ªæ)')
    parser.add_argument('--by-smallest', action='store_true', help='å‰ªè£å˜åŒ–æœ€å°çš„é€šé“ (é»˜è®¤)')
    parser.add_argument('--by-largest', action='store_false', dest='by_smallest', help='å‰ªè£å˜åŒ–æœ€å¤§çš„é€šé“')
    parser.add_argument('--output-dir', type=str, default='./results_pruning_only_deltaW', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--compute-gpu', type=int, default=2, help='ç”¨äºè®¡ç®—çš„GPU ID')
    parser.add_argument('--orig-gpu', type=int, default=1, help='åŠ è½½åŸå§‹æ¨¡å‹çš„GPU ID')
    parser.add_argument('--fine-gpu', type=int, default=3, help='åŠ è½½å¾®è°ƒæ¨¡å‹çš„GPU ID')
    parser.add_argument('--skip-orig-model', action='store_true', help='è·³è¿‡åŠ è½½åŸå§‹æ¨¡å‹(ç”¨äºè°ƒè¯•)')
    parser.add_argument('--skip-fine-model', action='store_true', help='è·³è¿‡åŠ è½½å¾®è°ƒæ¨¡å‹(ç”¨äºè°ƒè¯•)')
    parser.set_defaults(by_smallest=True)
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    model_orig = None
    model_finetuned = None
    
    # åŠ è½½ç¬¬ä¸€ä¸ªæ¨¡å‹
    if not args.skip_orig_model:
        print(f"å›ºå®šä½¿ç”¨GPU {args.orig_gpu}åŠ è½½ç¬¬ä¸€ä¸ªæ¨¡å‹")
        model_orig = load_prismatic_vlm(args.orig_gpu)
    else:
        print("è·³è¿‡åŠ è½½åŸå§‹æ¨¡å‹ (è°ƒè¯•æ¨¡å¼)")
        
    # åŠ è½½ç¬¬äºŒä¸ªæ¨¡å‹
    if not args.skip_fine_model:
        print(f"å›ºå®šä½¿ç”¨GPU {args.fine_gpu}åŠ è½½ç¬¬äºŒä¸ªæ¨¡å‹")
        model_finetuned = load_openvla(args.fine_gpu)
    else:
        print("è·³è¿‡åŠ è½½å¾®è°ƒæ¨¡å‹ (è°ƒè¯•æ¨¡å¼)")

    # æ‰“å°å¹¶ä¿å­˜æƒé‡ä¿¡æ¯
    save_model_weights_info(model_orig,  os.path.join(args.output_dir, "orig_model_weights.txt"))
    save_model_weights_info(model_finetuned, os.path.join(args.output_dir, "fine_model_weights.txt"))
    
    # æ£€æŸ¥æ¨¡å‹åŠ è½½æƒ…å†µ
    if (not args.skip_orig_model and model_orig is None) or (not args.skip_fine_model and model_finetuned is None):
        print("è­¦å‘Šï¼šè‡³å°‘ä¸€ä¸ªæ¨¡å‹åŠ è½½å¤±è´¥ã€‚")
        
        # è¿›å…¥æ¨¡æ‹Ÿè°ƒè¯•æ¨¡å¼
        print("æ˜¯å¦è¿›å…¥æ¨¡æ‹Ÿè°ƒè¯•æ¨¡å¼ç»§ç»­åˆ†æï¼Ÿ(è¾“å…¥yè¡¨ç¤ºæ˜¯ï¼Œä»»æ„é”®è¡¨ç¤ºå¦)")
        response = input().strip().lower()
        
        if response != 'y':
            print("é€€å‡ºç¨‹åºã€‚")
            return
        
        print("è¿›å…¥æ¨¡æ‹Ÿè°ƒè¯•æ¨¡å¼ï¼Œä½¿ç”¨éšæœºæƒé‡æ¨¡æ‹Ÿæ¨¡å‹...")
        
        # åˆ›å»ºç®€å•æ¨¡å‹ç»“æ„è¿›è¡Œè°ƒè¯•
        if model_orig is None and not args.skip_orig_model:
            from torch import nn
            model_orig = nn.Sequential(
                nn.Linear(768, 3072),
                nn.GELU(),
                nn.Linear(3072, 768),
            )
            print("åˆ›å»ºäº†æ¨¡æ‹ŸåŸå§‹æ¨¡å‹")
        
        if model_finetuned is None and not args.skip_fine_model:
            from torch import nn
            model_finetuned = nn.Sequential(
                nn.Linear(768, 3072),
                nn.GELU(),
                nn.Linear(3072, 768),
            )
            print("åˆ›å»ºäº†æ¨¡æ‹Ÿå¾®è°ƒæ¨¡å‹")
    
    # è®°å½•æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
    if model_orig is not None:
        orig_device = f"cuda:{args.orig_gpu}" if args.orig_gpu != "cpu" else "cpu"
        print(f"åŸå§‹æ¨¡å‹åœ¨è®¾å¤‡: {orig_device}")
    
    if model_finetuned is not None:
        fine_device = f"cuda:{args.fine_gpu}" if args.fine_gpu != "cpu" else "cpu"
        print(f"å¾®è°ƒæ¨¡å‹åœ¨è®¾å¤‡: {fine_device}")
    
    # æå–æ¨¡å‹ä¿¡æ¯
    if model_orig is not None:
        model_orig_info = extract_model_info(model_orig, "åŸå§‹æ¨¡å‹")
    else:
        model_orig_info = None
        
    if model_finetuned is not None:
        model_finetuned_info = extract_model_info(model_finetuned, "å¾®è°ƒæ¨¡å‹")
    else:
        model_finetuned_info = None
    
    if model_orig_info and model_finetuned_info:
        print(f"=== æ¨¡å‹å‚æ•°æ•°é‡æ¯”è¾ƒ ===")
        print(f"{model_orig_info['name']} - æ€»å‚æ•°é‡: {model_orig_info['num_parameters']}, å¯è®­ç»ƒå‚æ•°é‡: {model_orig_info['trainable_parameters']}")
        print(f"{model_finetuned_info['name']} - æ€»å‚æ•°é‡: {model_finetuned_info['num_parameters']}, å¯è®­ç»ƒå‚æ•°é‡: {model_finetuned_info['trainable_parameters']}")
    else:
        print("æ— æ³•æå–å®Œæ•´çš„æ¨¡å‹ä¿¡æ¯ï¼Œè·³è¿‡å‚æ•°æ¯”è¾ƒã€‚")
    
    # å›ºå®šä½¿ç”¨æŒ‡å®šGPUè¿›è¡Œè®¡ç®—
    compute_device = f"cuda:{args.compute_gpu}"
    print(f"å›ºå®šä½¿ç”¨ {compute_device} è¿›è¡Œå‚æ•°å·®å¼‚è®¡ç®—")


    analyze_and_generate_ffn_masks(
            model_orig, 
            model_finetuned, 
            compute_device, 
            output_dir = args.output_dir,
            ratio=args.ratio, 
            by_smallest=args.by_smallest,
            prune_target=args.prune_target
        )
    

if __name__ == "__main__":
    main()