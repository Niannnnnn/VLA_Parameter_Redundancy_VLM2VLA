import torch
import os
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import re
import pandas as pd
import argparse
import seaborn as sns
from utils import * 

def evaluate_vla_importance(mapping_list, model_orig, model_finetuned, output_dir, 
                            component="llm", target_module="ffn", attn_mode="channel",
                            metric_type="relative", compute_device="cuda:0", save_detail=False):
    """
    è¯„ä¼°é€šé“/å¤´éƒ¨é‡è¦æ€§ã€‚
    ä¿®å¤äº† LLM FFN ç»´åº¦å¯¹åº”é”™è¯¯ï¼ˆåº”è¯¥æ˜¯ 11008 ç»´åº¦ï¼‰å’Œ Key å‰ç¼€ä¸¢å¤±çš„é—®é¢˜ã€‚
    """
    print(f"ğŸ” è¯„ä¼°ç»„ä»¶: [{component}] | ç›®æ ‡: [{target_module}] | æ¨¡å¼: [{attn_mode}] | æŒ‡æ ‡: [{metric_type}] ...")
    device = torch.device(compute_device)
    plm_state = model_orig.state_dict()
    olm_state = model_finetuned.state_dict()
    importance_scores = {}

    # 1. å®šä¹‰å…³é”®å­—è¿‡æ»¤
    if component == "llm":
        target_keywords = ["gate_proj", "up_proj", "down_proj"] if target_module == "ffn" else ["q_proj"]
    elif "vision" in component:
        target_keywords = ["mlp.fc1"] if target_module == "ffn" else ["attn.qkv"]
    elif component == "projector":
        target_keywords = ["fc1", "fc2", "projector.0", "projector.2"] 
    else:
        target_keywords = ["weight"]

    # ç”¨äºå­˜æ”¾ LLM FFN æ¯ä¸€å±‚å„å­æ¨¡å—çš„å¾—åˆ†
    ffn_accumulator = defaultdict(dict)

    for (plm_name, olm_name) in mapping_list:
        if "# NOT FOUND" in olm_name: continue
        if "bias" in olm_name: continue 
        if not any(x in olm_name for x in target_keywords): continue

        # åŠ è½½æƒé‡
        W_orig = plm_state[plm_name].to(device).float()
        W_fine = olm_state[olm_name].to(device).float()
        delta_W = W_fine - W_orig
        
        # --- æ ¸å¿ƒä¿®å¤ 1: ç¡®å®šè®¡ç®—ç»´åº¦çš„æ–¹å‘ ---
        if component == "llm":
            if target_module == "ffn":
                # gate_proj (11008, 4096) -> å‰ªè¾“å‡º (dim 0)ï¼Œè®¡ç®— dim 1
                # up_proj   (11008, 4096) -> å‰ªè¾“å‡º (dim 0)ï¼Œè®¡ç®— dim 1
                # down_proj (4096, 11008) -> å‰ªè¾“å…¥ (dim 1)ï¼Œè®¡ç®— dim 0
                calc_dim = 0 if "down_proj" in olm_name else 1 
            else:
                # Attention: q_proj (4096, 4096) -> å‰ªè¾“å‡º (dim 0)ï¼Œè®¡ç®— dim 1
                calc_dim = 1 if "o_proj" in olm_name else 1
        elif "vision" in component or component == "projector":
            calc_dim = 1 
        else:
            calc_dim = 0 
        
        # 3. è®¡ç®—åŸºç¡€å¾—åˆ†
        # è¿™é‡Œçš„ calc_dim æ˜¯â€œè¢«èšåˆæ‰çš„ç»´åº¦â€ï¼Œä¿ç•™ä¸‹æ¥çš„å°±æ˜¯é‡è¦æ€§å‘é‡
        # ä¾‹å¦‚ gate_proj (11008, 4096) å¯¹ dim 1 æ±‚ normï¼Œå¾—åˆ° [11008]
        diff_norm = torch.norm(delta_W, p=2, dim=calc_dim)
        if metric_type == "relative":
            orig_norm = torch.norm(W_orig, p=2, dim=calc_dim) + 1e-8
            score = diff_norm / orig_norm
        else:
            score = diff_norm

        # --- ç‰¹æ®Šå¤„ç†ï¼šLLM FFN èšåˆé€»è¾‘ ---
        if component == "llm" and target_module == "ffn":
            match = re.search(r'layers\.(\d+)\.', olm_name)
            if match:
                l_idx = int(match.group(1))
                sub_module_name = next(x for x in ["gate_proj", "up_proj", "down_proj"] if x in olm_name)
                ffn_accumulator[l_idx][sub_module_name] = score.detach().cpu()
            continue 
        
        # --- å…¶ä»–æ¨¡å—ï¼ˆVision/Projector/LLM Attnï¼‰ ---
        if target_module == "attention" and attn_mode == "head":
            hidden_size = W_orig.shape[1]
            num_heads = 16 if hidden_size in [1024, 1152] else 32
            head_dim = hidden_size // num_heads
            if "qkv" in olm_name:
                score = score.view(3, num_heads, head_dim).mean(dim=[0, 2])
            else:
                score = score.view(num_heads, head_dim).mean(dim=1)

        importance_scores[olm_name] = score.detach().cpu()

    # --- æ ¸å¿ƒä¿®å¤ 2: èšåˆä¸ Key å‰ç¼€ä¿ç•™ ---
    if component == "llm" and target_module == "ffn":
        print(f"åˆå¹¶ LLM FFN å­æ¨¡å—å¾—åˆ† (Gate + Up + Down)...")
        
        # å»ºç«‹ç´¢å¼•ï¼Œç¡®ä¿è·å–å¸¦ 'module.language_model.' ç­‰å®Œæ•´å‰ç¼€çš„ gate_proj åç§°
        full_name_map = {}
        for _, olm_name in mapping_list:
            if "gate_proj" in olm_name:
                match = re.search(r'layers\.(\d+)\.', olm_name)
                if match:
                    full_name_map[int(match.group(1))] = olm_name

        for l_idx, sub_scores in ffn_accumulator.items():
            if not sub_scores: continue
            
            # èšåˆä¸‰ä¸ªæŠ•å½±å±‚åœ¨ 11008 ç»´åº¦ä¸Šçš„å¾—åˆ†
            combined_score = sum(sub_scores.values())
            
            if l_idx in full_name_map:
                rep_name = full_name_map[l_idx]
            else:
                # æœ€åçš„å…œåº•
                rep_name = f"module.language_model.model.layers.{l_idx}.mlp.gate_proj.weight"
            
            importance_scores[rep_name] = combined_score

    # 5. ä¿å­˜ç»Ÿè®¡æ•°æ®
    os.makedirs(output_dir, exist_ok=True)
    stats_path = os.path.join(output_dir, f"importance_stats_{component}_{target_module}_{metric_type}.csv")
    records = []
    for k, v in importance_scores.items():
        records.append({
            "Layer_Name": k, 
            "Min": v.min().item(), 
            "Max": v.max().item(), 
            "Mean": v.mean().item(), 
            "Count": len(v)
        })
    pd.DataFrame(records).to_csv(stats_path, index=False)
    
    return importance_scores

def generate_pruning_masks(delta_data, output_dir, ratio=0.2, by_smallest=True, 
                            component="llm", target_module="ffn", attn_mode="channel", metric_type="relative"):
    if not delta_data: return {}
    
    # ==========================================================
    # å±€éƒ¨é…ç½®ï¼šå±‚ä¿æŠ¤å¼€å…³
    # ==========================================================
    is_protected = False  # æ‰‹åŠ¨ä¿®æ”¹æ­¤å¤„ï¼šTrue å¼€å¯ä¿æŠ¤ï¼ŒFalse å…³é—­ä¿æŠ¤
    # protected_layers = [0, 1, 2, 31] # LLM éœ€è¦ä¿æŠ¤çš„å±‚å·
    protected_layers = [0, 1, 2, 3, 4, 30, 31] # LLM éœ€è¦ä¿æŠ¤çš„å±‚å·
    # protected_layers = [0, 1, 2, 3, 4, 5, 29, 30, 31] # LLM éœ€è¦ä¿æŠ¤çš„å±‚å·
    # ==========================================================

    pruning_masks = {}
    layer_scores = defaultdict(list)
    layer_to_keys = defaultdict(list)
    
    # 1. è§£æå±‚å·
    for k, v in delta_data.items():
        if component == "projector":
            if "fc1" in k or "projector.0" in k: l_idx = 1
            elif "fc2" in k or "projector.2" in k: l_idx = 2
            elif "fc3" in k or "projector.4" in k: l_idx = 3
            else: l_idx = 0
        else:
            match = re.search(r'(layers|blocks)\.(\d+)\.', k)
            l_idx = int(match.group(2)) if match else 0
        
        # ä¿æŠ¤é€»è¾‘ï¼šå¦‚æœæ˜¯ LLM ç»„ä»¶ä¸”å¼€å¯äº†ä¿æŠ¤ï¼Œå°†æŒ‡å®šå±‚çš„å¾—åˆ†è®¾ä¸ºæ— ç©·å¤§ï¼ˆæˆ–æå°å€¼ï¼‰
        # è¿™æ ·åœ¨å…¨å±€æ’åºæ—¶ï¼Œè¿™äº›å•å…ƒæ°¸è¿œä¸ä¼šè¢«é€‰ä¸­å‰ªæ
        v_to_sort = v.clone()
        if is_protected and component == "llm" and l_idx in protected_layers:
            if by_smallest:
                # å¦‚æœæ˜¯åˆ æ‰å¾—åˆ†æœ€å°çš„ï¼Œæˆ‘ä»¬å°±æŠŠä¿æŠ¤å±‚å¾—åˆ†è®¾ä¸ºæå¤§
                v_to_sort = torch.full_like(v, float('inf'))
            else:
                # å¦‚æœæ˜¯åˆ æ‰å¾—åˆ†æœ€å¤§çš„ï¼Œæˆ‘ä»¬å°±æŠŠä¿æŠ¤å±‚å¾—åˆ†è®¾ä¸ºæå°
                v_to_sort = torch.full_like(v, float('-inf'))
        
        layer_scores[l_idx].append(v_to_sort)
        layer_to_keys[l_idx].append(k)

    # 2. å…¨å±€æ’åº
    sorted_layer_ids = sorted(layer_scores.keys())
    all_scores_list = [torch.cat(layer_scores[i]) for i in sorted_layer_ids]
    flat_scores = torch.cat(all_scores_list)
    num_total = len(flat_scores)
    num_to_prune = int(num_total * ratio)
    
    strategy_str = 'smallest' if by_smallest else 'biggest'
    print(f"\n" + "="*60)
    print(f"ğŸ“Š å‰ªæä»»åŠ¡è¯¦æƒ… [{component.upper()}]: æ€»å•å…ƒæ•° = {num_total} | ç­–ç•¥ = {strategy_str} | å±‚ä¿æŠ¤ = {is_protected}")
    if is_protected and component == "llm":
        print(f"ğŸ›¡ï¸ å·²ä¿æŠ¤ LLM å±‚: {protected_layers}")
    print("="*60)

    _, sorted_indices = torch.sort(flat_scores, descending=False)
    prune_indices = sorted_indices[:num_to_prune] if by_smallest else sorted_indices[-num_to_prune:]
    global_mask_bool = torch.ones(num_total, dtype=torch.bool)
    global_mask_bool[prune_indices] = False

    # 3. æ‹†è§£å¹¶åˆ†å‘ (é€»è¾‘ä¿æŒä¸å˜)
    current_pos = 0
    for idx in sorted_layer_ids:
        n_elements = len(torch.cat(layer_scores[idx]))
        layer_mask = global_mask_bool[current_pos : current_pos + n_elements].numpy()
        current_pos += n_elements
        
        pruned_count_val = int(np.sum(~layer_mask))
        # æ‰“å°ä¿¡æ¯è¾…åŠ©éªŒè¯
        prot_tag = " [PROTECTED]" if is_protected and component == "llm" and idx in protected_layers else ""
        print(f"Layer {idx:<2} | Pruned: {pruned_count_val:<5} | Total: {n_elements:<5}{prot_tag}")

        for k in layer_to_keys[idx]:
            k_clean = k.replace("module.", "")
            base_meta = {
                'layer_num': idx,
                'pruned_count': pruned_count_val,
                'total_count': n_elements,
                'target_module': target_module,
                'attn_mode': attn_mode,
                'component': component
            }

            # --- ä»¥ä¸‹æ˜¯æ‚¨åŸæœ‰çš„åˆ†å‘é€»è¾‘ (LLM, Vision, Projector) ---
            if component == "llm":
                if target_module == "ffn":
                    for suffix in ["gate_proj", "up_proj", "down_proj"]:
                        target_k = k_clean.replace("gate_proj", suffix)
                        is_down = "down_proj" in target_k
                        pruning_masks[target_k] = {
                            **base_meta,
                            'output_mask': None if is_down else layer_mask,
                            'input_mask': layer_mask if is_down else None,
                        }
                else:
                    h_dim = 128 
                    actual_mask = np.repeat(layer_mask, h_dim)
                    for suffix in ["q_proj", "k_proj", "v_proj", "o_proj"]:
                        target_k = k_clean.replace("q_proj", suffix)
                        is_o = "o_proj" in target_k
                        pruning_masks[target_k] = {
                            **base_meta,
                            'total_count': len(actual_mask),
                            'pruned_count': int(np.sum(~actual_mask)),
                            'output_mask': None if is_o else actual_mask,
                            'input_mask': actual_mask if is_o else None,
                        }
            elif "vision" in component and target_module == "ffn":
                pruning_masks[k_clean] = {**base_meta, 'output_mask': layer_mask, 'input_mask': None}
                fc2_key = k_clean.replace("fc1", "fc2")
                pruning_masks[fc2_key] = {**base_meta, 'output_mask': None, 'input_mask': layer_mask}
            elif component == "projector":
                pruning_masks[k_clean] = {**base_meta, 'output_mask': layer_mask, 'input_mask': None}
                if "fc" in k_clean: next_k = k_clean.replace(f"fc{idx}", f"fc{idx+1}")
                else: next_k = k_clean.replace(f"projector.{(idx-1)*2}", f"projector.{(idx)*2}")
                if next_k not in pruning_masks:
                    pruning_masks[next_k] = {**base_meta, 'layer_num': idx+1, 'output_mask': None, 'input_mask': layer_mask}
                else:
                    pruning_masks[next_k]['input_mask'] = layer_mask
            else:
                is_input_side = any(x in k for x in ["attn.proj", "proj.weight"])
                actual_mask = layer_mask
                if target_module == "attention" and attn_mode == "head":
                    h_dim = 72 if "siglip" in component else (64 if "dino" in component else 128)
                    actual_mask = np.repeat(layer_mask, h_dim)
                pruning_masks[k_clean] = {
                    **base_meta,
                    'total_count': len(actual_mask),
                    'pruned_count': int(np.sum(~actual_mask)),
                    'output_mask': None if is_input_side else actual_mask,
                    'input_mask': actual_mask if is_input_side else None,
                }

    # 4. ä¿å­˜
    prot_suffix = f"_protected_L{'_L'.join(map(str, protected_layers))}" if (is_protected and component == "llm") else ""
    save_name = f"masks_{component}_{target_module}_{attn_mode}_{ratio}_{strategy_str}_{metric_type}{prot_suffix}.pth"
    torch.save(pruning_masks, os.path.join(output_dir, save_name))
    print(f"ğŸ’¾ æ©ç æ–‡ä»¶å·²ä¿å­˜è‡³: {os.path.join(output_dir, save_name)}\n" + "="*60 + "\n")
    return pruning_masks

# def plot_importance_heatmap(delta_data, output_dir, component="llm", target_module="ffn", attn_mode="channel", metric_type="relative"):
#     if not delta_data: return
#     print(f"ğŸ“Š æ­£åœ¨ç”Ÿæˆ {component} çƒ­åŠ›å›¾...")
    
#     layer_map = defaultdict(list)
#     for k, v in delta_data.items():
#         if component == "projector":
#             idx = 1 if ("fc1" in k or "projector.0" in k) else (2 if ("fc2" in k or "projector.2" in k) else 3)
#         else:
#             match = re.search(r'(layers|blocks)\.(\d+)\.', k)
#             idx = int(match.group(2)) if match else 0
#         layer_map[idx].append(v)
    
#     sorted_idx = sorted(layer_map.keys())
#     max_len = max([len(torch.cat(layer_map[i])) for i in sorted_idx])
    
#     processed_rows = []
#     for i in sorted_idx:
#         row_data = torch.cat(layer_map[i]).numpy()
#         if len(row_data) < max_len:
#             padded_row = np.full(max_len, np.nan)
#             padded_row[:len(row_data)] = row_data
#             processed_rows.append(padded_row)
#         else:
#             processed_rows.append(row_data)
    
#     heatmap_matrix = np.array(processed_rows)
#     plt.figure(figsize=(15, 6))
#     sns.heatmap(heatmap_matrix, cmap="YlGnBu", robust=True, 
#                 vmax=np.percentile(heatmap_matrix[~np.isnan(heatmap_matrix)], 98),
#                 mask=np.isnan(heatmap_matrix), rasterized=True)
    
#     # plt.title(f"{component.upper()} {target_module} Importance ({metric_type})")
#     plt.yticks(np.arange(len(sorted_idx)) + 0.5, sorted_idx, rotation=0)
#     # save_fig = f"heatmap_{component}_{target_module}_{attn_mode}_{metric_type}.png"
#     save_fig = f"heatmap_{component}_{target_module}_{attn_mode}.pdf"
#     # plt.savefig(os.path.join(output_dir, save_fig), bbox_inches='tight', dpi=300)
#     plt.savefig(os.path.join(output_dir, save_fig), bbox_inches='tight')
#     plt.close()




from matplotlib.ticker import MaxNLocator

def plot_importance_heatmap(delta_data, output_dir, component="llm", target_module="ffn", attn_mode="channel", metric_type="relative"):
    if not delta_data: return
    print(f"ğŸ“Š æ­£åœ¨ç”Ÿæˆ {component} çƒ­åŠ›å›¾ (è®ºæ–‡æ ¼å¼)...")
    
    # --- æ ·å¼å¸¸é‡é…ç½® ---
    LABEL_SIZE = 18    # è½´æ ‡é¢˜å­—å·
    TICK_SIZE = 14     # åˆ»åº¦æ•°å­—å­—å·
    CBAR_SIZE = 14     # é¢œè‰²æ¡å­—å·
    
    # 1. æ•°æ®å¯¹é½ä¸é¢„å¤„ç† (ä¿æŒä½ çš„åŸæœ‰é€»è¾‘)
    layer_map = defaultdict(list)
    for k, v in delta_data.items():
        if component == "projector":
            idx = 1 if ("fc1" in k or "projector.0" in k) else (2 if ("fc2" in k or "projector.2" in k) else 3)
        else:
            match = re.search(r'(layers|blocks)\.(\d+)\.', k)
            idx = int(match.group(2)) if match else 0
        layer_map[idx].append(v)
    
    sorted_idx = sorted(layer_map.keys())
    max_len = max([len(torch.cat(layer_map[i])) for i in sorted_idx])
    
    processed_rows = []
    for i in sorted_idx:
        row_data = torch.cat(layer_map[i]).numpy()
        if len(row_data) < max_len:
            padded_row = np.full(max_len, np.nan)
            padded_row[:len(row_data)] = row_data
            processed_rows.append(padded_row)
        else:
            processed_rows.append(row_data)
    
    # 2. ç»˜å›¾æ ¸å¿ƒéƒ¨åˆ†
    heatmap_matrix = np.array(processed_rows)
    plt.figure(figsize=(12, 5)) # ç•¥å¾®ç¼©å°å®½åº¦ï¼Œæé«˜æ–‡å­—åœ¨ PDF ä¸­çš„ç›¸å¯¹å æ¯”
    
    # ä½¿ç”¨ xticklabels=False é¿å… Seaborn å°è¯•æ¸²æŸ“ä¸Šä¸‡ä¸ªæ ‡ç­¾
    ax = sns.heatmap(
        heatmap_matrix, 
        cmap="YlGnBu", 
        robust=True, 
        vmax=np.percentile(heatmap_matrix[~np.isnan(heatmap_matrix)], 98),
        mask=np.isnan(heatmap_matrix), 
        rasterized=True,
        xticklabels=False, 
        cbar_kws={'shrink': 0.8}
    )
    
    # 3. åæ ‡è½´ä¸æ ‡ç­¾ç¾åŒ–
    plt.xlabel("Channel / Head Index", fontsize=LABEL_SIZE, labelpad=10)
    plt.ylabel("Layer Index", fontsize=LABEL_SIZE, labelpad=10)

    # çºµè½´ï¼šå±‚å·åˆ»åº¦
    # å¦‚æœå±‚æ•°è¾ƒå¤šï¼ˆå¦‚ 32 å±‚ï¼‰ï¼Œæ¯ 5 å±‚æ˜¾ç¤ºä¸€ä¸ªï¼›å¦‚æœå¾ˆå°‘ï¼ˆå¦‚ Projectorï¼‰ï¼Œåˆ™å…¨éƒ¨æ˜¾ç¤º
    y_step = 5 if len(sorted_idx) > 10 else 1
    y_indices = np.arange(0, len(sorted_idx), y_step)
    plt.yticks(y_indices + 0.5, [sorted_idx[i] for i in y_indices], 
               rotation=0, fontsize=TICK_SIZE)

    # æ¨ªè½´ï¼šä½¿ç”¨ MaxNLocator è‡ªåŠ¨æ§åˆ¶ 5-6 ä¸ªåˆ»åº¦ï¼Œå½»åº•è§£å†³ FFN ä¸‡çº§é€šé“é‡å é—®é¢˜
    num_channels = heatmap_matrix.shape[1]
    locator = MaxNLocator(nbins=5, integer=True)
    x_ticks = locator.tick_values(0, num_channels)
    x_ticks = [t for t in x_ticks if t < num_channels] # è¿‡æ»¤æ‰è¶Šç•Œçš„åˆ»åº¦
    
    plt.xticks(np.array(x_ticks) + 0.5, [f"{int(t)}" for t in x_ticks], 
               rotation=0, fontsize=TICK_SIZE)

    # 4. é¢œè‰²æ¡å­—å·è°ƒæ•´
    cbar = ax.collections[0].colorbar
    cbar.ax.tick_params(labelsize=CBAR_SIZE)
    
    # 5. ä¿å­˜
    save_fig = f"heatmap_{component}_{target_module}_{attn_mode}.pdf"
    plt.savefig(os.path.join(output_dir, save_fig), bbox_inches='tight')
    plt.close()
    print(f"âœ… å·²æˆåŠŸä¿å­˜ PDF çƒ­åŠ›å›¾: {save_fig}")



def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--component', type=str, default='vision_siglip', choices=['llm', 'vision_dino', 'vision_siglip', 'projector'])
    parser.add_argument('--target-module', type=str, default='attention', choices=['ffn', 'attention'])
    parser.add_argument('--attn-mode', type=str, default='head', choices=['channel', 'head'])
    parser.add_argument('--ratio', type=float, default=0.2)
    parser.add_argument('--by-biggest', action='store_false', dest='by_smallest')
    parser.add_argument('--metric-type', type=str, default='relative', choices=['absolute', 'relative'])
    parser.add_argument('--output-dir', type=str, default='./analysis_results')
    parser.add_argument('--compute-gpu', type=int, default=0)
    parser.add_argument('--orig-gpu', type=int, default=1)
    parser.add_argument('--fine-gpu', type=int, default=2)
    parser.set_defaults(by_smallest=True)
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)
    model_orig = load_prismatic_vlm(args.orig_gpu)
    model_finetuned = load_openvla(args.fine_gpu)
    
    mapping_list = match_parameters(model_orig, model_finetuned, args.output_dir, component=args.component)
    
    importance_data = evaluate_vla_importance(
        mapping_list, model_orig, model_finetuned, args.output_dir,
        component=args.component, target_module=args.target_module,
        attn_mode=args.attn_mode, metric_type=args.metric_type,
        compute_device=f"cuda:{args.compute_gpu}"
    )

    plot_importance_heatmap(importance_data, args.output_dir, args.component, args.target_module, args.attn_mode, args.metric_type)

    generate_pruning_masks(
        importance_data, args.output_dir, ratio=args.ratio, by_smallest=args.by_smallest, 
        component=args.component, target_module=args.target_module,
        attn_mode=args.attn_mode, metric_type=args.metric_type
    )

if __name__ == "__main__":
    main()