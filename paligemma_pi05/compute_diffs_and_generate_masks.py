import torch
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import defaultdict
from pathlib import Path
from datetime import datetime
from safetensors.torch import load_file
from transformers import AutoModel

# ==========================================
# é…ç½®ä¸è·¯å¾„
# ==========================================
# VLA_PATH = Path("/home/intern/zhangfengnian/checkpoints/pi05_libero_pytorch/model.safetensors")
VLA_PATH = Path("/mnt/afs/huangtao/intern/zhangfengnian/checkpoints/pi05_libero_pytorch/model.safetensors")
VLM_NAME = "google/paligemma-3b-mix-224"
OUTPUT_DIR = Path("./analysis_results_pi05")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# PaLiGemma 3B æ¶æ„å‚æ•°
V_HIDDEN = 1152
V_HEADS = 16
L_HIDDEN = 2048
L_Q_HEADS = 8
L_KV_HEADS = 1

# ==========================================
# æ ¸å¿ƒè®¡ç®—å‡½æ•°
# ==========================================

def get_importance_score(vla_weight, vlm_weight, calc_dim=1):
    """è®¡ç®— L2 Norm å·®å¼‚"""
    delta_w = vla_weight.to(torch.float32) - vlm_weight.to(torch.float32)
    score = torch.norm(delta_w, p=2, dim=calc_dim) 
    return score.cpu().numpy()

def process_pi05_importance():
    print("ğŸš€ æ­£åœ¨åŠ è½½æƒé‡å¹¶è®¡ç®—å·®å¼‚ (VLM vs VLA)...")
    vla_sd = load_file(VLA_PATH)
    vlm_model = AutoModel.from_pretrained(VLM_NAME, torch_dtype=torch.float16)
    vlm_sd = vlm_model.state_dict()

    importance_data = {
        "vision_ffn": {}, "vision_attn": {},
        "llm_ffn": {}, "llm_attn": {}
    }

    # 1. Vision Tower (27å±‚)
    for i in range(27):
        vla_pre = f"paligemma_with_expert.paligemma.model.vision_tower.vision_model.encoder.layers.{i}"
        vlm_pre = f"vision_tower.vision_model.encoder.layers.{i}"
        
        # FFN (fc1)
        importance_data["vision_ffn"][f"layer.{i}"] = get_importance_score(
            vla_sd[f"{vla_pre}.mlp.fc1.weight"], vlm_sd[f"{vlm_pre}.mlp.fc1.weight"])

        # ATTN (MHA) -> èšåˆä¸º Head
        q_score = get_importance_score(
            vla_sd[f"{vla_pre}.self_attn.q_proj.weight"], vlm_sd[f"{vlm_pre}.self_attn.q_proj.weight"])
        importance_data["vision_attn"][f"layer.{i}"] = q_score.reshape(V_HEADS, -1).mean(axis=1)

    # 2. Language Model (18å±‚)
    for i in range(18):
        vla_pre = f"paligemma_with_expert.paligemma.model.language_model.layers.{i}"
        vlm_pre = f"language_model.layers.{i}"

        # FFN (gate_proj)
        importance_data["llm_ffn"][f"layer.{i}"] = get_importance_score(
            vla_sd[f"{vla_pre}.mlp.gate_proj.weight"], vlm_sd[f"{vlm_pre}.mlp.gate_proj.weight"])

        # ATTN (GQA) -> èšåˆä¸º Head Group
        q_score = get_importance_score(
            vla_sd[f"{vla_pre}.self_attn.q_proj.weight"], vlm_sd[f"{vlm_pre}.self_attn.q_proj.weight"])
        k_score = get_importance_score(
            vla_sd[f"{vla_pre}.self_attn.k_proj.weight"], vlm_sd[f"{vlm_pre}.self_attn.k_proj.weight"])
        
        q_heads = q_score.reshape(L_Q_HEADS, -1).mean(axis=1)
        importance_data["llm_attn"][f"layer.{i}"] = (q_heads + k_score.mean()) / 2

    return importance_data

# ==========================================
# è‡ªåŠ¨åŒ–æ©ç ç”Ÿæˆä¸åŠ¨æ€å‘½å
# ==========================================

def generate_pruning_masks(importance_dict, ratio=0.2, by_smallest=True, component="llm_ffn"):
    """
    by_smallest=True: å‰ªæ‰å·®å¼‚æœ€å°çš„ (ä¿ç•™æ ¸å¿ƒå˜åŒ–)
    by_smallest=False: å‰ªæ‰å·®å¼‚æœ€å¤§çš„ (æµ‹è¯•æ€§èƒ½å—æŸ)
    """
    all_scores = []
    layer_keys = sorted(importance_dict.keys(), key=lambda x: int(x.split('.')[-1]))
    for k in layer_keys: all_scores.append(importance_dict[k])
    
    flat_scores = np.concatenate(all_scores)
    num_total = len(flat_scores)
    num_to_prune = int(num_total * ratio)
    
    # ç­–ç•¥ç¡®å®š
    sorted_indices = np.argsort(flat_scores)
    strategy = "smallest" if by_smallest else "biggest"
    
    if by_smallest:
        prune_indices = sorted_indices[:num_to_prune]
    else:
        prune_indices = sorted_indices[-num_to_prune:]
        
    global_mask = np.ones(num_total, dtype=bool)
    global_mask[prune_indices] = False
    
    # æ„å»ºåŒ…å«å…ƒæ•°æ®çš„ä¸°å¯Œä¿¡æ¯å­—å…¸
    pruning_results = {
        "metadata": {
            "component": component,
            "ratio": ratio,
            "strategy": strategy,
            "total_units": num_total,
            "pruned_units": num_to_prune,
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S")
        },
        "layers": {}
    }
    
    curr = 0
    for k in layer_keys:
        n = len(importance_dict[k])
        layer_mask = global_mask[curr : curr + n]
        curr += n
        pruning_results["layers"][k] = {
            "mask": layer_mask,
            "pruned_count": int(np.sum(~layer_mask)),
            "kept_count": int(np.sum(layer_mask)),
            "total_count": n
        }

    # è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
    filename = f"masks_{component}_r{ratio}_{strategy}.pth"
    save_path = OUTPUT_DIR / filename
    torch.save(pruning_results, save_path)
    
    print(f"âœ… ä¿å­˜æ©ç : {filename} | å‰ªææ¯”ä¾‹: {ratio*100:.1f}% | ç­–ç•¥: {strategy}")
    return pruning_results

# ==========================================
# å¯è§†åŒ– (OpenVLA é£æ ¼)
# ==========================================
'''
def plot_pi05_heatmaps(data):
    titles = {
        "vision_ffn": "Vision Tower FFN (fc1) Importance",
        "vision_attn": "Vision Tower Attention (Head) Importance",
        "llm_ffn": "Language Model FFN (gate) Importance",
        "llm_attn": "Language Model Attention (GQA) Importance"
    }
    
    for key, title in titles.items():
        layers = sorted(data[key].keys(), key=lambda x: int(x.split('.')[-1]))
        matrix = np.array([data[key][l] for l in layers])
        
        plt.figure(figsize=(15, 6))
        sns.heatmap(matrix, cmap="YlGnBu", robust=True, vmax=np.percentile(matrix, 98), rasterized=True)
        # plt.title(f"Pi0.5: {title}")
        plt.xlabel("Channel / Head Index")
        plt.ylabel("Layer Index")
        # plt.savefig(OUTPUT_DIR / f"heatmap_{key}.png", bbox_inches='tight', dpi=300)
        save_path = OUTPUT_DIR / f"heatmap_{key}.pdf"
        plt.savefig(save_path, bbox_inches='tight') # PDF ä¸éœ€è¦æŒ‡å®š dpi
        plt.close()
    print(f"ğŸ“Š çƒ­åŠ›å›¾å·²ä¿å­˜è‡³ {OUTPUT_DIR}")
'''


from matplotlib.ticker import MaxNLocator

def plot_pi05_heatmaps(data):
    LABEL_SIZE = 18 
    TICK_SIZE = 14  
    CBAR_SIZE = 14  

    keys = ["vision_ffn", "vision_attn", "llm_ffn", "llm_attn"]
    
    for key in keys:
        if key not in data or not data[key]: continue
        
        layers = sorted(data[key].keys(), key=lambda x: int(x.split('.')[-1]))
        matrix = np.array([data[key][l] for l in layers])
        
        plt.figure(figsize=(12, 5))
        
        # 1. ç»˜å›¾ï¼šè®¾ç½® xticklabels=False è®© Seaborn ä¸è¦è‡ªåŠ¨ç”Ÿæˆå¯†é›†çš„åˆ»åº¦
        ax = sns.heatmap(
            matrix, 
            cmap="YlGnBu", 
            robust=True, 
            vmax=np.percentile(matrix, 98), 
            rasterized=True,
            xticklabels=False, # æš‚æ—¶å…³é—­ï¼Œç”±æˆ‘ä»¬æ‰‹åŠ¨ç²¾å‡†æ§åˆ¶
            cbar_kws={'shrink': 0.8}
        )
        
        plt.xlabel("Channel / Head Index", fontsize=LABEL_SIZE, labelpad=10)
        plt.ylabel("Layer Index", fontsize=LABEL_SIZE, labelpad=10)

        # 2. çºµè½´åˆ»åº¦æ§åˆ¶ (æ¯ 5 å±‚æ˜¾ç¤ºä¸€ä¸ª)
        y_step = 5
        y_indices = np.arange(0, len(layers), y_step)
        plt.yticks(y_indices + 0.5, [int(layers[i].split('.')[-1]) for i in y_indices], 
                   rotation=0, fontsize=TICK_SIZE)
        
        # 3. æ¨ªè½´åˆ»åº¦æ§åˆ¶ (é‡ç‚¹æ”¹è¿›)
        num_channels = matrix.shape[1]
        
        # è‡ªåŠ¨è®¡ç®—æ­¥é•¿ï¼šç›®æ ‡æ˜¯æ˜¾ç¤º 5-6 ä¸ªåˆ»åº¦
        # ä½¿ç”¨ MaxNLocator è‡ªåŠ¨å¯»æ‰¾å¦‚ 2000, 4000 è¿™æ ·â€œæ¼‚äº®â€çš„æ•´åˆ†ä½
        locator = MaxNLocator(nbins=5, integer=True)
        x_ticks = locator.tick_values(0, num_channels)
        
        # è¿‡æ»¤æ‰è¶…å‡ºè¾¹ç•Œçš„åˆ»åº¦
        x_ticks = [t for t in x_ticks if t < num_channels]
        
        # è®¾ç½®åˆ»åº¦ä½ç½®å’Œæ ‡ç­¾
        # å¦‚æœç»´åº¦å¾ˆå¤§ï¼Œå¯ä»¥å°†æ ‡ç­¾æ ¼å¼åŒ–ä¸º '2k', '4k' ç­‰ï¼Œæˆ–è€…ä¿æŒåŸæ ·
        plt.xticks(np.array(x_ticks) + 0.5, [f"{int(t)}" for t in x_ticks], 
                   rotation=0, fontsize=TICK_SIZE)

        # 4. é¢œè‰²æ¡å­—å·
        cbar = ax.collections[0].colorbar
        cbar.ax.tick_params(labelsize=CBAR_SIZE)
        
        save_path = OUTPUT_DIR / f"heatmap_{key}.pdf"
        plt.savefig(save_path, bbox_inches='tight')
        plt.close()
        
    print(f"ğŸ“Š æ”¹è¿›åçš„ PDF (å·²è§£å†³ FFN åˆ»åº¦é‡å ) å·²ä¿å­˜è‡³ {OUTPUT_DIR}")




# ==========================================
# æ‰§è¡Œå®éªŒå¾ªç¯
# ==========================================
if __name__ == "__main__":
    # 1. è®¡ç®—æ‰€æœ‰ç»„ä»¶çš„é‡è¦æ€§å¾—åˆ†
    scores = process_pi05_importance()
    
    # 2. ç”Ÿæˆæ‰€æœ‰ç»„ä»¶çš„çƒ­åŠ›å›¾
    plot_pi05_heatmaps(scores)
    
    # 3. è‡ªåŠ¨åŒ–å®éªŒå¾ªç¯ï¼šå®šä¹‰ä½ æƒ³æµ‹è¯•çš„æ¯”ä¾‹å’Œç­–ç•¥
    # test_ratios = [0.1, 0.2, 0.5]
    test_ratios = [0.1]
    test_strategies = [True, False] # True=Smallest, False=Biggest
    # target_components = ["llm_ffn", "llm_attn", "vision_ffn", "vision_attn"]
    target_components = ["llm_ffn", "llm_attn", "vision_ffn", "vision_attn"]

    print("\n" + "="*50 + "\nğŸ§ª å¼€å§‹ç”Ÿæˆå¤šå‚æ•°å‰ªæå®éªŒæ©ç ...\n" + "="*50)

    for comp in target_components:
        for r in test_ratios:
            for strat in test_strategies:
                generate_pruning_masks(
                    scores[comp], 
                    ratio=r, 
                    by_smallest=strat, 
                    component=comp
                )

    print("\n" + "="*50 + f"\nğŸ‰ åˆ†æä¸å®éªŒç”Ÿæˆå…¨éƒ¨å®Œæˆï¼\nç»“æœç›®å½•: {OUTPUT_DIR.absolute()}")